# ===- Pooling_manual.py -------------------------------------------------------
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# ===---------------------------------------------------------------------------
#
# This file implements the TVM manual optimization entry for Pooling on CPU.
# This file is based on the TVM tutorial:
# https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html
# TVM is an Apache-2.0 licensed project.
# See the TVM license at: https://github.com/apache/tvm/blob/main/LICENSE
#
# ===---------------------------------------------------------------------------

import tvm
from tvm import te
import numpy as np
import timeit
import mxnet as mx


def pool_mxnet(pool_type, data, out, k, p, s):
    mx.nd.Pooling(
        data, kernel=(k, k), stride=(s, s), pad=(p, p), pool_type=pool_type, out=out
    )


def bench_pooling_tvm(func, sizes, target):
    """Benchmark pooling in TVM
    func : the scheduling method
    sizes : the data size list, each of which is a (channel, input_hw, kernel_hw) triplet
    target : the TVM target, e.g. llvm or cuda
    """

    def workload(nrepeats):
        timer = mod.time_evaluator(mod.entry_name, ctx=ctx, number=nrepeats)
        return timer(data, out_max).mean * nrepeats

    times = []
    for size in sizes:
        sch, args = func(size)
        mod = tvm.build(sch, args, target)
        ctx = tvm.context(target, 0)
        data, _, out_max = get_conv_data(
            size[0], size[0], size[1], size[2], 1, 1, lambda x: tvm.nd.array(x, ctx=ctx)
        )
        times.append(bench_workload(workload))
    return np.array(times)


def pooling_timer_mxnet(pool_type, c, n, k, ctx):
    """Benchmark pooling in MXNet
    c : channels
    n : input width and height
    k : kernel width and height
    """
    timer = timeit.Timer(
        setup="import mxnet as mx\n"
        "from Pooling_manual import get_pool_data_mxnet\n"
        "from Pooling_manual import pool_mxnet\n"
        "c, n, k, p, s = %d, %d, %d, 1, 1\n"
        "data, out = get_pool_data_mxnet(\n"
        '    c, n, k, p, s, "%s")' % (c, n, k, ctx),
        stmt='pool_mxnet("%s", data, out, k, p, s);' "out.wait_to_read()" % (pool_type),
    )
    return timer.timeit


def bench_pooling_mxnet(pool_type, sizes, ctx="cpu"):
    """Return the execution times of MXNet pooling"""
    return bench_workload(
        pooling_timer_mxnet(pool_type, sizes[0], sizes[1], sizes[2], ctx)
    )


def bench_workload(workload):
    """Benchmark a workload
    workload: a method that accept a num_repeat argument
    and return its total execution time
    """
    workload(1)  # warmup
    time = workload(1)  # the time to run once
    if time > 1:
        return time
    # The number of repeats to measure at least 1 second
    num_repeats = max(int(1.0 / time), 5)
    return workload(num_repeats) / num_repeats


def get_conv_data(oc, ic, n, k, p=0, s=1, constructor=None):
    """Return random 3-D data tensor, 3-D kernel tenor and empty 3-D output
    tensor with the shapes specified by input arguments.
    oc, ic : output and input channels
    n : input width and height
    k : kernel width and height
    p : padding size, default 0
    s : stride, default 1
    constructor : user-defined tensor constructor
    """
    np.random.seed(0)
    data = np.random.normal(size=(ic, n, n)).astype("float32")
    weight = np.random.normal(size=(oc, ic, k, k)).astype("float32")
    on = conv_out_size(n, k, p, s)
    out = np.empty((oc, on, on), dtype="float32")
    if constructor:
        data, weight, out = (constructor(x) for x in [data, weight, out])
    return data, weight, out


def padding(X, ph, pw, val=0):
    """Pad X with the given value in 2-D
    ph, pw : height and width padding
    val : padding value, default 0
    """
    assert len(X.shape) >= 2
    nh, nw = X.shape[-2], X.shape[-1]
    return te.compute(
        (*X.shape[0:-2], nh + ph * 2, nw + pw * 2),
        lambda *i: te.if_then_else(
            te.any(i[-2] < ph, i[-2] >= nh + ph, i[-1] < pw, i[-1] >= nw + pw),
            val,
            X[i[:-2] + (i[-2] - ph, i[-1] - pw)],
        ),
        name="PaddedX",
    )


def conv_out_size(n, k, p, s):
    """Compute the output size by given input size n (width or height),
    kernel size k, padding p, and stride s
    Return output size (width or height)
    """
    return (n - k + 2 * p) // s + 1


def pool(pool_type, c, nh, nw, kh, kw, ph=0, pw=0, sh=1, sw=1):
    """2D pooling
    pool_type: pooling type, 'max' or 'avg'
    c : channels
    nh, nw : input width and height
    kh, kw : kernel width and height
    ph, pw : height and width padding sizes, default 0
    sh, sw : height and width strides, default 1
    """
    # reduction axes
    rkh = te.reduce_axis((0, kh), name="rkh")
    rkw = te.reduce_axis((0, kw), name="rkw")
    # output height and weights
    oh = conv_out_size(nh, kh, ph, sh)
    ow = conv_out_size(nw, kw, pw, sw)
    # pad X and then compute Y
    X = te.placeholder((c, nh, nw), name="X")
    if pool_type == "max":
        PaddedX = padding(X, ph, pw, val=te.min_value(X.dtype)) if ph * pw != 0 else X
        Y = te.compute(
            (c, oh, ow),
            lambda c, h, w: te.max(
                PaddedX[c, h * sh + rkh, w * sw + rkw], axis=[rkh, rkw]
            ),
            tag="pool_max",
            name="PoolMax",
        )
    elif pool_type == "avg":
        PaddedX = padding(X, ph, pw) if ph * pw != 0 else X
        tsum = te.compute(
            (c, oh, ow),
            lambda c, h, w: te.sum(
                PaddedX[c, h * sh + rkh, w * sw + rkw], axis=[rkh, rkw]
            ),
            tag="pool_avg1",
            name="PoolSum",
        )
        Y = te.compute(
            (c, oh, ow),
            lambda c, h, w: tsum[c, h, w] / (kh * kw),
            tag="pool_avg2",
            name="PoolAvg",
        )
    else:
        raise ValueError("Pool type should be 'avg' or 'max'.")
    return X, Y, PaddedX


def default_max(size):
    c, n, k = size[:]
    X, Y, PaddedX = pool("max", c, n, n, k, k, 1, 1, 1, 1)
    sch = te.create_schedule(Y.op)
    return sch, (X, Y)


def optimized_max(size):
    sch, (X, Y) = default_max(size)
    te.schedule.AutoInlineInjective(sch)
    c, h, w = Y.op.axis[0:3]
    fused = sch[Y].fuse(c, h)
    sch[Y].parallel(fused)
    sch[Y].vectorize(w)
    return sch, (X, Y)


def get_pool_data_mxnet(c, n, k, p, s, ctx="cpu"):
    ctx = getattr(mx, ctx)()
    data, _, out = get_conv_data(c, c, n, k, p, s, lambda x: mx.nd.array(x, ctx=ctx))
    data, out = data.expand_dims(axis=0), out.expand_dims(axis=0)
    return data, out


def main():
    size = (64, 64, 3)
    c, n, k, p, s = size[0], size[0], size[1], size[2], 1
    data, _, out_max = get_conv_data(
        size[0], size[0], size[1], size[2], 1, 1, tvm.nd.array
    )
    sch, arg_bufs = default_max(size)
    mod(data, out_max)


if __name__ == "__main__":
    main()
